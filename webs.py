import requests
import os
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import concurrent.futures
import re
import json

class UltimateWebsiteDownloader:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.downloaded_files = set()
        self.folder_name = ""
        self.base_url = ""
        self.domain = ""
        self.visited_urls = set()
    
    def get_user_input(self):
        """рдпреВрдЬрд░ рд╕реЗ URL рдФрд░ рдлреЛрд▓реНрдбрд░ рдирд╛рдо рд▓реЗрдВ"""
        print("ЁЯМР рдЕрд▓реНрдЯреАрдореЗрдЯ рд╡реЗрдмрд╕рд╛рдЗрдЯ рдбрд╛рдЙрдирд▓реЛрдбрд░")
        print("=" * 50)
        print("рдХрд┐рд╕реА рднреА рд╡реЗрдмрд╕рд╛рдЗрдЯ рдХреА рд╕рднреА рдлрд╛рдЗрд▓реНрд╕ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ")
        print("=" * 50)
        
        website_url = input("рд╡реЗрдмрд╕рд╛рдЗрдЯ рдХрд╛ рдкреВрд░рд╛ URL рдбрд╛рд▓реЗрдВ: ").strip()
        if not website_url:
            print("тЭМ URL рдЬрд░реВрд░реА рд╣реИ!")
            return None, None
        
        if not website_url.startswith(('http://', 'https://')):
            website_url = 'https://' + website_url
        
        folder_name = input("рд╕реЗрд╡ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдлреЛрд▓реНрдбрд░ рдирд╛рдо рдбрд╛рд▓реЗрдВ: ").strip()
        if not folder_name:
            folder_name = urlparse(website_url).netloc.replace('www.', '') + "_website"
        
        return website_url, folder_name
    
    def download_complete_website(self, website_url, folder_name):
        """рдкреВрд░реА рд╡реЗрдмрд╕рд╛рдЗрдЯ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ"""
        self.base_url = website_url
        self.domain = urlparse(website_url).netloc
        self.folder_name = folder_name
        
        print(f"\nЁЯЪА рдбрд╛рдЙрдирд▓реЛрдб рд╢реБрд░реВ: {website_url}")
        print(f"ЁЯУБ рдлреЛрд▓реНрдбрд░: {folder_name}")
        print("=" * 60)
        
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
        
        # рд╕реНрдЯреЗрдк 1: рдореБрдЦреНрдп рдкреЗрдЬ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ рдФрд░ рд╕рднреА рд▓рд┐рдВрдХреНрд╕ рдвреВрдВрдвреЗрдВ
        print("ЁЯУД рдореБрдЦреНрдп рдкреЗрдЬ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░ рд░рд╣рд╛ рд╣реВрдБ...")
        all_urls = self.recursive_crawl(website_url, max_depth=3)
        
        # рд╕реНрдЯреЗрдк 2: рд╕рднреА рдлрд╛рдЗрд▓реНрд╕ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ
        self.download_all_resources(all_urls)
        
        print(f"\nтЬЕ рдбрд╛рдЙрдирд▓реЛрдб рдкреВрд░рд╛! рдХреБрд▓ {len(self.downloaded_files)} рдлрд╛рдЗрд▓реНрд╕")
        self.show_report()
    
    def recursive_crawl(self, start_url, max_depth=3):
        """Recursive рддрд░реАрдХреЗ рд╕реЗ рдкреВрд░реА рд╡реЗрдмрд╕рд╛рдЗрдЯ рдХреНрд░реЙрд▓ рдХрд░реЗрдВ"""
        print(f"ЁЯХ╕я╕П Recursive crawling рд╢реБрд░реВ (max depth: {max_depth})...")
        
        all_urls = set()
        to_crawl = [(start_url, 0)]  # (url, depth)
        
        while to_crawl:
            current_url, depth = to_crawl.pop(0)
            
            if current_url in self.visited_urls or depth > max_depth:
                continue
                
            self.visited_urls.add(current_url)
            print(f"   ЁЯФН Depth {depth}: {self.get_display_url(current_url)}")
            
            try:
                # URL рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ
                response = self.session.get(current_url, timeout=15)
                if response.status_code == 200:
                    # рдлрд╛рдЗрд▓ рд╕реЗрд╡ рдХрд░реЗрдВ
                    self.save_file(current_url, response.content, response.headers.get('content-type', ''))
                    
                    # HTML рдкреЗрдЬ рд╣реИ рддреЛ рд▓рд┐рдВрдХреНрд╕ рдирд┐рдХрд╛рд▓реЗрдВ
                    if 'text/html' in response.headers.get('content-type', ''):
                        new_urls = self.extract_all_links_from_content(response.text, current_url)
                        
                        # рдирдП URLs рдЬреЛрдбрд╝реЗрдВ
                        for url in new_urls:
                            if url not in self.visited_urls and url not in [u for u, d in to_crawl]:
                                if self.should_crawl(url, depth):
                                    to_crawl.append((url, depth + 1))
                                all_urls.add(url)
                    
                    all_urls.add(current_url)
                    
            except Exception as e:
                print(f"   тЭМ рдХреНрд░реЙрд▓ рддреНрд░реБрдЯрд┐: {e}")
            
            time.sleep(0.3)  # рд╕рд░реНрд╡рд░ рдХреЛ рдУрд╡рд░рд▓реЛрдб рди рдХрд░реЗрдВ
        
        print(f"   ЁЯУК рдХреБрд▓ {len(all_urls)} URLs рдорд┐рд▓реЗ")
        return all_urls
    
    def extract_all_links_from_content(self, content, base_url):
        """рдХрдВрдЯреЗрдВрдЯ рд╕реЗ рд╕рднреА рд▓рд┐рдВрдХреНрд╕ рдирд┐рдХрд╛рд▓реЗрдВ (HTML + JavaScript)"""
        urls = set()
        
        # HTML рд▓рд┐рдВрдХреНрд╕
        soup = BeautifulSoup(content, 'html.parser')
        
        # рд╕рднреА HTML рдЯреИрдЧреНрд╕
        html_tags = [
            ('a', 'href'),
            ('link', 'href'),
            ('script', 'src'),
            ('img', 'src'),
            ('source', 'src'),
            ('audio', 'src'),
            ('video', 'src'),
            ('iframe', 'src'),
            ('form', 'action'),
            ('meta', 'content')
        ]
        
        for tag_name, attr in html_tags:
            for tag in soup.find_all(tag_name, {attr: True}):
                url = tag.get(attr)
                if url:
                    full_url = self.normalize_url(url, base_url)
                    if self.is_same_domain(full_url):
                        urls.add(full_url)
        
        # CSS рдореЗрдВ URLs
        css_urls = re.findall(r'url\([\'"]?([^\'")]+)[\'"]?\)', content)
        for css_url in css_urls:
            full_url = self.normalize_url(css_url, base_url)
            if self.is_same_domain(full_url):
                urls.add(full_url)
        
        # JavaScript рдореЗрдВ URLs - рдПрдбрд╡рд╛рдВрд╕реНрдб рдбрд┐рдЯреЗрдХреНрд╢рди
        js_urls = self.extract_urls_from_javascript(content, base_url)
        urls.update(js_urls)
        
        return list(urls)
    
    def extract_urls_from_javascript(self, content, base_url):
        """JavaScript рдХреЛрдб рд╕реЗ URLs рдирд┐рдХрд╛рд▓реЗрдВ"""
        urls = set()
        
        # fetch() рдФрд░ XMLHttpRequest calls
        fetch_patterns = [
            r'fetch\([\'"]([^\'"]+)[\'"]\)',
            r'\.open\([\'"]GET[\'"],\s*[\'"]([^\'"]+)[\'"]\)',
            r'\.open\([\'"]POST[\'"],\s*[\'"]([^\'"]+)[\'"]\)',
            r'axios\.(?:get|post)\([\'"]([^\'"]+)[\'"]\)',
            r'\.get\([\'"]([^\'"]+)[\'"]\)',
            r'\.post\([\'"]([^\'"]+)[\'"]\)',
        ]
        
        for pattern in fetch_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                full_url = self.normalize_url(match, base_url)
                if self.is_same_domain(full_url):
                    urls.add(full_url)
        
        # URL strings in JavaScript
        url_patterns = [
            r'[\'\"](/[^\'\"\s]+\.(?:html|css|js|json|txt|xml))[\'\"]',
            r'[\'\"](\./[^\'\"\s]+\.(?:html|css|js|json|txt|xml))[\'\"]',
            r'[\'\"](\.\.[^\'\"\s]+\.(?:html|css|js|json|txt|xml))[\'\"]',
            r'[\'\"]([^\'\"\s]+/[\w\-]+\.(?:html|css|js|json|txt|xml))[\'\"]',
        ]
        
        for pattern in url_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                full_url = self.normalize_url(match, base_url)
                if self.is_same_domain(full_url):
                    urls.add(full_url)
        
        # JSON data рдореЗрдВ URLs
        json_patterns = [
            r'[\'"]url[\'"]\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'[\'"]src[\'"]\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'[\'"]href[\'"]\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'[\'"]file[\'"]\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'[\'"]path[\'"]\s*:\s*[\'"]([^\'"]+)[\'"]',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                full_url = self.normalize_url(match, base_url)
                if self.is_same_domain(full_url):
                    urls.add(full_url)
        
        # Array рдореЗрдВ URLs (рдЬреИрд╕реЗ batch files)
        array_patterns = [
            r'=\s*\[(.*?)\]',
            r'const\s+\w+\s*=\s*\[(.*?)\]',
            r'let\s+\w+\s*=\s*\[(.*?)\]',
            r'var\s+\w+\s*=\s*\[(.*?)\]',
        ]
        
        for pattern in array_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                # Array items рдирд┐рдХрд╛рд▓реЗрдВ
                items = re.findall(r'[\'"]([^\'"]+)[\'"]', match)
                for item in items:
                    if any(ext in item for ext in ['.html', '.css', '.js', '.txt', '.json', '.xml']):
                        full_url = self.normalize_url(item, base_url)
                        if self.is_same_domain(full_url):
                            urls.add(full_url)
        
        return list(urls)
    
    def should_crawl(self, url, depth):
        """рдЪреЗрдХ рдХрд░реЗрдВ рдХрд┐ URL рдХреЛ рдХреНрд░реЙрд▓ рдХрд░рдирд╛ рдЪрд╛рд╣рд┐рдП"""
        # рд╕рд┐рд░реНрдл same domain
        if not self.is_same_domain(url):
            return False
        
        # рд╕рд┐рд░реНрдл HTML рдкреЗрдЬреЗрд╕ рдХреЛ рдХреНрд░реЙрд▓ рдХрд░реЗрдВ (рдФрд░ рдХреБрдЫ specific рдлрд╛рдЗрд▓ рдЯрд╛рдЗрдкреНрд╕)
        if url.endswith(('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', 
                        '.pdf', '.zip', '.mp4', '.mp3', '.woff', '.woff2', '.ttf')):
            return False
        
        # рдкреИрд░рд╛рдореАрдЯрд░реНрд╕ рд╡рд╛рд▓реЗ URLs рдХреЛ рд╕реАрдорд┐рдд рдХрд░реЗрдВ
        if '?' in url and depth > 1:
            return False
        
        return True
    
    def should_download(self, url):
        """рдЪреЗрдХ рдХрд░реЗрдВ рдХрд┐ рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдиреА рдЪрд╛рд╣рд┐рдП"""
        # рд╕рд┐рд░реНрдл same domain рдХреА рдлрд╛рдЗрд▓реНрд╕
        if not self.is_same_domain(url):
            return False
        
        # рдмрд╣реБрдд рдмрдбрд╝реА рдлрд╛рдЗрд▓реНрд╕ рдХреЛ рд╕реНрдХрд┐рдк рдХрд░реЗрдВ
        if any(ext in url.lower() for ext in ['.mp4', '.avi', '.mov', '.mkv', '.webm']):
            return False
        
        return True
    
    def download_all_resources(self, urls):
        """рд╕рднреА рд░рд┐рд╕реЛрд░реНрд╕реЗрдЬ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ"""
        # рдлрд┐рд▓реНрдЯрд░ URLs
        download_urls = [url for url in urls if self.should_download(url) and url not in self.downloaded_files]
        
        if not download_urls:
            print("тД╣я╕П рдбрд╛рдЙрдирд▓реЛрдб рдХреЗ рд▓рд┐рдП рдХреЛрдИ рдирдИ рдлрд╛рдЗрд▓реНрд╕ рдирд╣реАрдВ рдорд┐рд▓реАрдВ")
            return
        
        print(f"\nЁЯУе {len(download_urls)} рдлрд╛рдЗрд▓реНрд╕ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░ рд░рд╣рд╛ рд╣реВрдБ...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            for url in download_urls:
                future = executor.submit(self.download_single_file, url)
                futures.append(future)
            
            completed = 0
            total = len(futures)
            
            for future in concurrent.futures.as_completed(futures):
                completed += 1
                if completed % 10 == 0 or completed == total:
                    print(f"   ЁЯУК {completed}/{total} рдбрд╛рдЙрдирд▓реЛрдб рд╣реЛ рдЪреБрдХреЗ...")
    
    def download_single_file(self, url):
        """рд╕рд┐рдВрдЧрд▓ рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ"""
        try:
            response = self.session.get(url, timeout=15)
            
            if response.status_code == 200:
                self.save_file(url, response.content, response.headers.get('content-type', ''))
                self.downloaded_files.add(url)
            elif response.status_code == 404:
                print(f"   тЭМ {self.get_filename(url)} - рдирд╣реАрдВ рдорд┐рд▓реА (404)")
            else:
                print(f"   тЪая╕П {self.get_filename(url)} - рд╕реНрдЯреЗрдЯрд╕: {response.status_code}")
                
        except Exception as e:
            print(f"   тЭМ {self.get_filename(url)} - рддреНрд░реБрдЯрд┐: {e}")
    
    def save_file(self, url, content, content_type=""):
        """рдлрд╛рдЗрд▓ рдХреЛ рд╕реЗрд╡ рдХрд░реЗрдВ"""
        try:
            file_path = self.get_local_path(url)
            
            # рдлреЛрд▓реНрдбрд░ рдмрдирд╛рдПрдВ
            file_dir = os.path.dirname(file_path)
            if file_dir and not os.path.exists(file_dir):
                os.makedirs(file_dir, exist_ok=True)
            
            # рдХреЙрдиреНрдЯреЗрдВрдЯ рдЯрд╛рдЗрдк рдХреЗ рдЕрдиреБрд╕рд╛рд░ рд╕реЗрд╡ рдХрд░реЗрдВ
            if isinstance(content, str):
                content = content.encode('utf-8')
            
            # рдЯреЗрдХреНрд╕реНрдЯ рдлрд╛рдЗрд▓реНрд╕ рдХреЗ рд▓рд┐рдП UTF-8 encoding
            if any(ct in content_type for ct in ['text/', 'application/javascript', 'application/json']) or \
               url.endswith(('.html', '.htm', '.css', '.js', '.txt', '.json', '.xml')):
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content.decode('utf-8'))
                except:
                    with open(file_path, 'wb') as f:
                        f.write(content)
            else:
                with open(file_path, 'wb') as f:
                    f.write(content)
            
            file_size = len(content)
            display_path = os.path.relpath(file_path, self.folder_name)
            print(f"     тЬЕ {display_path} ({file_size} bytes)")
            
        except Exception as e:
            print(f"     тЭМ рд╕реЗрд╡ рдХрд░рдиреЗ рдореЗрдВ рддреНрд░реБрдЯрд┐: {self.get_filename(url)} - {e}")
    
    def get_local_path(self, url):
        """URL рдХреЛ рд▓реЛрдХрд▓ рдлрд╛рдЗрд▓ рдкрд╛рде рдореЗрдВ рдХрдиреНрд╡рд░реНрдЯ рдХрд░реЗрдВ"""
        parsed = urlparse(url)
        path = parsed.path
        
        if not path or path == '/':
            return os.path.join(self.folder_name, "index.html")
        
        # рд░реВрдЯ рдкрд╛рде рдХреЛ рд╕рд╛рдл рдХрд░реЗрдВ
        path = path.lstrip('/')
        
        # рдбрд╛рдпрд░реЗрдХреНрдЯрд░реА рдХреЗ рд▓рд┐рдП index.html рдПрдб рдХрд░реЗрдВ
        if not path or path.endswith('/'):
            path = os.path.join(path, "index.html")
        elif '.' not in os.path.basename(path):
            path += ".html"
        
        return os.path.join(self.folder_name, path)
    
    def normalize_url(self, url, base_url):
        """URL рдХреЛ рдкреВрд░рд╛ URL рдореЗрдВ рдХрдиреНрд╡рд░реНрдЯ рдХрд░реЗрдВ"""
        if url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            return urljoin(self.base_url, url)
        elif url.startswith('./'):
            return urljoin(base_url, url)
        elif url.startswith('../'):
            return urljoin(base_url, url)
        elif not url.startswith('http'):
            return urljoin(base_url, url)
        else:
            return url
    
    def is_same_domain(self, url):
        """рдЪреЗрдХ рдХрд░реЗрдВ рдХрд┐ URL same domain рдХрд╛ рд╣реИ"""
        try:
            return urlparse(url).netloc == self.domain
        except:
            return False
    
    def get_display_url(self, url):
        """рдбрд┐рд╕реНрдкреНрд▓реЗ рдХреЗ рд▓рд┐рдП рдЫреЛрдЯрд╛ URL рдмрдирд╛рдПрдВ"""
        return url.replace(self.base_url, '') or '/'
    
    def get_filename(self, url):
        """URL рд╕реЗ рдлрд╛рдЗрд▓рдирд╛рдо рдирд┐рдХрд╛рд▓реЗрдВ"""
        return os.path.basename(urlparse(url).path) or "index.html"
    
    def show_report(self):
        """рдбрд╛рдЙрдирд▓реЛрдб рд░рд┐рдкреЛрд░реНрдЯ рджрд┐рдЦрд╛рдПрдВ"""
        print("\n" + "=" * 60)
        print("ЁЯУК рдЕрд▓реНрдЯреАрдореЗрдЯ рд╡реЗрдмрд╕рд╛рдЗрдЯ рдбрд╛рдЙрдирд▓реЛрдб рд░рд┐рдкреЛрд░реНрдЯ")
        print("=" * 60)
        
        if not os.path.exists(self.folder_name):
            print("тЭМ рдлреЛрд▓реНрдбрд░ рдирд╣реАрдВ рдмрдирд╛!")
            return
        
        file_count = 0
        total_size = 0
        file_types = {}
        
        for root, dirs, files in os.walk(self.folder_name):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file)[1].lower() or 'no-ext'
                file_size = os.path.getsize(file_path)
                
                file_types[file_ext] = file_types.get(file_ext, 0) + 1
                total_size += file_size
                file_count += 1
        
        print(f"ЁЯМР рд╡реЗрдмрд╕рд╛рдЗрдЯ: {self.base_url}")
        print(f"ЁЯУБ рдлреЛрд▓реНрдбрд░: {self.folder_name}")
        print(f"ЁЯУД рдХреБрд▓ рдлрд╛рдЗрд▓реНрд╕: {file_count}")
        print(f"ЁЯТ╛ рдХреБрд▓ рд╕рд╛рдЗрдЬ: {total_size / 1024 / 1024:.2f} MB")
        
        print("\nЁЯУЛ рдлрд╛рдЗрд▓ рдЯрд╛рдЗрдкреНрд╕:")
        for ext, count in sorted(file_types.items()):
            if count > 0:
                print(f"   {ext or 'no-ext'}: {count} рдлрд╛рдЗрд▓реНрд╕")
        
        # рдореБрдЦреНрдп рдлрд╛рдЗрд▓реНрд╕ рдХреА рд▓рд┐рд╕реНрдЯ
        main_files = []
        for root, dirs, files in os.walk(self.folder_name):
            for file in files:
                if file in ['index.html', 'main.html', 'app.html', 'home.html']:
                    main_files.append(os.path.join(root, file))
        
        if main_files:
            print(f"\nЁЯПа рдореБрдЦреНрдп рдлрд╛рдЗрд▓реНрд╕:")
            for main_file in main_files:
                rel_path = os.path.relpath(main_file, self.folder_name)
                print(f"   ЁЯУД {rel_path}")
        
        print(f"\nтЬЕ рдбрд╛рдЙрдирд▓реЛрдб рдкреВрд░рд╛ рд╣реЛ рдЧрдпрд╛!")
        print(f"ЁЯУН рд▓реЛрдХреЗрд╢рди: {os.path.abspath(self.folder_name)}")
        
        # рдУрдкрди рдХрд░рдиреЗ рдХрд╛ рд╕реБрдЭрд╛рд╡
        index_path = os.path.join(self.folder_name, "index.html")
        if os.path.exists(index_path):
            print(f"ЁЯМР рд╡реЗрдмрд╕рд╛рдЗрдЯ рджреЗрдЦрдиреЗ рдХреЗ рд▓рд┐рдП: file://{os.path.abspath(index_path)}")

def main():
    downloader = UltimateWebsiteDownloader()
    
    try:
        website_url, folder_name = downloader.get_user_input()
        
        if website_url and folder_name:
            print(f"\nтЪб рдкреНрд░реЛрд╕реЗрд╕ рд╢реБрд░реВ рдХрд░ рд░рд╣рд╛ рд╣реВрдБ...")
            downloader.download_complete_website(website_url, folder_name)
        else:
            print("тЭМ рдЗрдирдкреБрдЯ рд╡реИрдз рдирд╣реАрдВ рд╣реИ!")
            
    except KeyboardInterrupt:
        print("\nтП╣я╕П рдбрд╛рдЙрдирд▓реЛрдб рд░реЛрдХ рджрд┐рдпрд╛ рдЧрдпрд╛")
    except Exception as e:
        print(f"\nтЭМ рддреНрд░реБрдЯрд┐: {e}")

if __name__ == "__main__":
    # рд▓рд╛рдЗрдмреНрд░реЗрд░реАрдЬ рдЪреЗрдХ рдХрд░реЗрдВ
    try:
        import requests
        from bs4 import BeautifulSoup
    except ImportError:
        print("тЭМ рдЬрд░реВрд░реА рд▓рд╛рдЗрдмреНрд░реЗрд░реАрдЬ рдЗрдВрд╕реНрдЯреЙрд▓ рдирд╣реАрдВ рд╣реИрдВ!")
        print("рдЗрдВрд╕реНрдЯреЙрд▓ рдХрд░реЗрдВ: pip install requests beautifulsoup4")
        exit(1)
    
    main()